{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "received-exercise",
   "metadata": {},
   "source": [
    "# 1.1 Neural network basics\n",
    "\n",
    "In this notebook, we construct a basic neural network in PyTorch. We attempt to only provide a bare-bone example for clarity. For another such example, please see [PyTorch's homepage](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "\n",
    "<!--\n",
    "- [Building the network](#Building-the-network)\n",
    "- [Training the network](#Training-the-network)\n",
    "- [Testing the network](#Testing-the-network)\n",
    "- [Exercises](#Exercises)\n",
    "-->\n",
    "\n",
    "First, we import the libraries and load the data. You will explore this further in notebook 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "otherwise-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fc1d56c1384589a58e521c2fed8b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a71c597e2ec4fa39da38081deadaeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1170d49cb41b4ceb8d8bb5e1805753c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4522e9342d4f60af01d4338bf08aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# We load a training set\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# And withhold some data for testing\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-official",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Below, we construct a fully connected neural network. Here, we will limit ourselves to a single hidden layer, i.e., it's a shallow rather than deep neural network. To construct a neural network in PyTorch, we define a class that inherits from the nn.Module class. \n",
    "\n",
    "The class contains two functions:\n",
    "\n",
    "1) init: Defines all the objects we will need.\n",
    "\n",
    "2) forward: puts all of these objects together, defining the network architecture. Basically, x starts out as your input tensor and is transformed step by step as it passes through each layer of the network. In this notebook, we consider MNIST images as our input tensors. Each image contains 28x28 pixels. The fully connected layers of the neural network, however, can only deal with one-dimensional feature vectors. So, we flatten the tensor, x, which is then passed on to the first fully connected layer, fc1. After the first fully connected layer, a Rectified Linear Unit (ReLU) activation function is applied element-wise to the tensor. The tensor is then passed through the second fully connected layer, fc2. After passing through the second fully connected layer, the transformed x is returned. The final tensor (x) represents the output of the neural network, which can be used for tasks like classification (you can rename x if you don't want to call the input and the output the same or if you want to distinguish between feature extraction and the final classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "individual-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-parts",
   "metadata": {},
   "source": [
    "You can now create an instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "banner-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-choice",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "We now want to train the weights of the neural network. For this purpose, we first need to define what a good fit to data entails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stupid-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-distribution",
   "metadata": {},
   "source": [
    "Secondly, we need to decide how we want to optimise the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "promising-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-string",
   "metadata": {},
   "source": [
    "Now, we are ready to train the neural network. Let's train for 5 epochs. Note that fixing the number of epochs in this manner is not a good choice and will lead to under- or overfitting. But in this notebook, we merely want to understand the basic concepts behind training a neural network.\n",
    "\n",
    "In each epoch, for each batch from the training set, we\n",
    "\n",
    "1) clear the accumulated gradients of the model parameters (accumulated during backpropagation),\n",
    "\n",
    "2) perform a forward run (i.e. we predict labels for the batch of images),\n",
    "\n",
    "3) compute the loss for these predictions,\n",
    "\n",
    "3) perform backpropagation,\n",
    "\n",
    "5) and use this to optimise the weights and biases of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structural-interpretation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.43157690029640994\n",
      "Epoch [2/5], Loss: 0.3554805261954665\n",
      "Epoch [3/5], Loss: 0.33244464016780256\n",
      "Epoch [4/5], Loss: 0.31666396109859146\n",
      "Epoch [5/5], Loss: 0.3237694415308535\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        outputs = model(images) # forward run\n",
    "        loss = criterion(outputs, labels) # compare to ground truth\n",
    "        loss.backward() # back propagation\n",
    "        optimizer.step() # update weights and biases\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-senegal",
   "metadata": {},
   "source": [
    "## Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-investing",
   "metadata": {},
   "source": [
    "Having trained the network, we can now test it on unseen data (the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "perfect-gravity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 90.18%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on the test set: {100 * accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-ethernet",
   "metadata": {},
   "source": [
    "That's a pretty high accuracy, reflecting that the MNIST dataset is relatively simple and well-behaved. Indeed, you can do much better. CNNs can reach an accuracy of more than 99 per cent (see [Kaggle](https://www.kaggle.com/code/cdeotte/how-to-choose-cnn-architecture-mnist)). To achieve such a high accuracy, we need to improve the architecture. We will do so in notebook 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-invite",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-training",
   "metadata": {},
   "source": [
    "**Exercise 1**: Here, we use the adam optimiser. Find out what other optimisers are available.\n",
    "\n",
    "**Exercise 2**: What do the hyperparameters (e.g. lr in adam) mean? Change lr to a higher value (e.g. 1) and rerun the notebook. Or to a lower value. What happens? Why?\n",
    "\n",
    "**Exercise 3**: Here, we use the cross entropy as the loss function. What is a loss function? And what is the cross entropy?\n",
    "\n",
    "**Exercise 4**: Integrate data augmentation techniques into the data preprocessing pipeline. This helps improve the model's generalisation.\n",
    "\n",
    "**Exercise 5**: Change the architecture of the neural network. Add more layers, increase the number of neurons, or try a different activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-franchise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
